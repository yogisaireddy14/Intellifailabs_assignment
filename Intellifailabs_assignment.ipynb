{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQSQTF3TfOJdAxW7lUPOBO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25061151c9a7419dbcf28b25f5f917b0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_3c8aad139c8c4a698e981bbfc2e27858",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Making motion templates... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Making motion templates... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "3c8aad139c8c4a698e981bbfc2e27858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37a72a5a651b472ca1a132146e3d625b": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_a2bbd48fecbe49f8822c79ad3b0cd141",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "🚀Animating... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:12\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">🚀Animating... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:12</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "a2bbd48fecbe49f8822c79ad3b0cd141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad6c5160bcd49e08138966b7f9f1530": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_6fb07c397b4e4d76ab7e5004261842d9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Concatenating result... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:00\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Concatenating result... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:00</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "6fb07c397b4e4d76ab7e5004261842d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87c69c0018b143c88ea80da0ed25a862": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_b23f94065e9c40ce87aa67c8a04a6fc2",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Writing \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Writing <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "b23f94065e9c40ce87aa67c8a04a6fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9aa42d3e610e4053a9c7f5b4c82ce118": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_4e9adadc45c34b1497334864b5fd3a8e",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "Writing \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:01\u001b[0m\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Writing <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:01</span>\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "4e9adadc45c34b1497334864b5fd3a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yogisaireddy14/Intellifailabs_assignment/blob/main/Intellifailabs_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn16IlSifCdS",
        "outputId": "061c2683-2450-4c8a-d095-44bf2a4c4eed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'LivePortrait' already exists and is not an empty directory.\n",
            "/content/LivePortrait\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/KwaiVGI/LivePortrait.git\n",
        "%cd LivePortrait"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UmdapVuxfMH6",
        "outputId": "7d2ad5db-c41d-4632-ae9f-8615087abd94"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.8 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.18.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==2.3.0\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.0) (11.2.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchvision, torchaudio\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.2.0\n",
            "\u001b[2K    Uninstalling triton-3.2.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.2.0\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torch-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.21.0+cu124\n",
            "\u001b[2K    Uninstalling torchvision-0.21.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchaudio\n",
            "\u001b[2K    Found existing installation: torchaudio 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torchaudio-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [torchaudio]\n",
            "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0+cu121 torchaudio-2.3.0+cu121 torchvision-0.18.0+cu121 triton-2.3.0\n",
            "Collecting numpy==1.26.4 (from -r /content/LivePortrait/requirements_base.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pyyaml==6.0.1 (from -r /content/LivePortrait/requirements_base.txt (line 2))\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting opencv-python==4.10.0.84 (from -r /content/LivePortrait/requirements_base.txt (line 3))\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting scipy==1.13.1 (from -r /content/LivePortrait/requirements_base.txt (line 4))\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting imageio==2.34.2 (from -r /content/LivePortrait/requirements_base.txt (line 5))\n",
            "  Downloading imageio-2.34.2-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting lmdb==1.4.1 (from -r /content/LivePortrait/requirements_base.txt (line 6))\n",
            "  Downloading lmdb-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Collecting tqdm==4.66.4 (from -r /content/LivePortrait/requirements_base.txt (line 7))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting rich==13.7.1 (from -r /content/LivePortrait/requirements_base.txt (line 8))\n",
            "  Downloading rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting ffmpeg-python==0.2.0 (from -r /content/LivePortrait/requirements_base.txt (line 9))\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting onnx==1.16.1 (from -r /content/LivePortrait/requirements_base.txt (line 10))\n",
            "  Downloading onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting scikit-image==0.24.0 (from -r /content/LivePortrait/requirements_base.txt (line 11))\n",
            "  Downloading scikit_image-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting albumentations==1.4.10 (from -r /content/LivePortrait/requirements_base.txt (line 12))\n",
            "  Downloading albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting matplotlib==3.9.0 (from -r /content/LivePortrait/requirements_base.txt (line 13))\n",
            "  Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting imageio-ffmpeg==0.5.1 (from -r /content/LivePortrait/requirements_base.txt (line 14))\n",
            "  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting tyro==0.8.5 (from -r /content/LivePortrait/requirements_base.txt (line 15))\n",
            "  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting gradio==5.1.0 (from -r /content/LivePortrait/requirements_base.txt (line 16))\n",
            "  Downloading gradio-5.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pykalman==0.9.7 (from -r /content/LivePortrait/requirements_base.txt (line 17))\n",
            "  Downloading pykalman-0.9.7-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pillow>=10.2.0 in /usr/local/lib/python3.11/dist-packages (from -r /content/LivePortrait/requirements_base.txt (line 18)) (11.2.1)\n",
            "Collecting onnxruntime-gpu==1.18.0 (from -r requirements.txt (line 3))\n",
            "  Downloading onnxruntime_gpu-1.18.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Collecting transformers==4.38.0 (from -r requirements.txt (line 4))\n",
            "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich==13.7.1->-r /content/LivePortrait/requirements_base.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich==13.7.1->-r /content/LivePortrait/requirements_base.txt (line 8)) (2.19.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python==0.2.0->-r /content/LivePortrait/requirements_base.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx==1.16.1->-r /content/LivePortrait/requirements_base.txt (line 10)) (5.29.5)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r /content/LivePortrait/requirements_base.txt (line 11)) (3.5)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r /content/LivePortrait/requirements_base.txt (line 11)) (2025.5.26)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r /content/LivePortrait/requirements_base.txt (line 11)) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image==0.24.0->-r /content/LivePortrait/requirements_base.txt (line 11)) (0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (4.13.2)\n",
            "Requirement already satisfied: scikit-learn>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (1.6.1)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (2.11.5)\n",
            "Requirement already satisfied: albucore>=0.0.11 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (4.11.0.86)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (2.9.0.post0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from imageio-ffmpeg==0.5.1->-r /content/LivePortrait/requirements_base.txt (line 14)) (75.2.0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro==0.8.5->-r /content/LivePortrait/requirements_base.txt (line 15)) (0.16)\n",
            "Collecting shtab>=1.5.6 (from tyro==0.8.5->-r /content/LivePortrait/requirements_base.txt (line 15))\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.5.0)\n",
            "Collecting gradio-client==1.4.0 (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16))\n",
            "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.32.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16))\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (3.10.18)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (2.2.2)\n",
            "Collecting pillow>=10.2.0 (from -r /content/LivePortrait/requirements_base.txt (line 18))\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.11.12)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.16.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.34.2)\n",
            "Collecting coloredlogs (from onnxruntime-gpu==1.18.0->-r requirements.txt (line 3))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (2.32.3)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0->-r requirements.txt (line 4))\n",
            "  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.38.0->-r requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.4.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (2025.3.2)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16))\n",
            "  Downloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (1.3.1)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.46.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.1->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (1.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.0->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (1.5.4)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore>=0.0.11->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore>=0.0.11->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (6.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==5.1.0->-r /content/LivePortrait/requirements_base.txt (line 16)) (0.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich==13.7.1->-r /content/LivePortrait/requirements_base.txt (line 8)) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0->-r /content/LivePortrait/requirements_base.txt (line 13)) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.2->albumentations==1.4.10->-r /content/LivePortrait/requirements_base.txt (line 12)) (3.6.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu==1.18.0->-r requirements.txt (line 3))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.38.0->-r requirements.txt (line 4)) (2.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu==1.18.0->-r requirements.txt (line 3)) (1.3.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m144.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio-2.34.2-py3-none-any.whl (313 kB)\n",
            "Downloading lmdb-1.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (302 kB)\n",
            "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "Downloading rich-13.7.1-py3-none-any.whl (240 kB)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading onnx-1.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m164.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_image-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m153.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albumentations-1.4.10-py3-none-any.whl (161 kB)\n",
            "Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n",
            "Downloading gradio-5.1.0-py3-none-any.whl (42.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pykalman-0.9.7-py2.py3-none-any.whl (251 kB)\n",
            "Downloading onnxruntime_gpu-1.18.0-cp311-cp311-manylinux_2_28_x86_64.whl (199.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-12.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Installing collected packages: lmdb, websockets, tqdm, tomlkit, shtab, pyyaml, pillow, numpy, markupsafe, imageio-ffmpeg, humanfriendly, ffmpeg-python, aiofiles, scipy, rich, opencv-python, onnx, imageio, coloredlogs, tyro, tokenizers, scikit-image, pykalman, onnxruntime-gpu, matplotlib, gradio-client, transformers, gradio, albumentations\n",
            "\u001b[2K  Attempting uninstall: websockets\n",
            "\u001b[2K    Found existing installation: websockets 15.0.1\n",
            "\u001b[2K    Uninstalling websockets-15.0.1:\n",
            "\u001b[2K      Successfully uninstalled websockets-15.0.1\n",
            "\u001b[2K  Attempting uninstall: tqdm\n",
            "\u001b[2K    Found existing installation: tqdm 4.67.1\n",
            "\u001b[2K    Uninstalling tqdm-4.67.1:\n",
            "\u001b[2K      Successfully uninstalled tqdm-4.67.1\n",
            "\u001b[2K  Attempting uninstall: tomlkit\n",
            "\u001b[2K    Found existing installation: tomlkit 0.13.2\n",
            "\u001b[2K    Uninstalling tomlkit-0.13.2:\n",
            "\u001b[2K      Successfully uninstalled tomlkit-0.13.2\n",
            "\u001b[2K  Attempting uninstall: pyyaml\n",
            "\u001b[2K    Found existing installation: PyYAML 6.0.2\n",
            "\u001b[2K    Uninstalling PyYAML-6.0.2:\n",
            "\u001b[2K      Successfully uninstalled PyYAML-6.0.2\n",
            "\u001b[2K  Attempting uninstall: pillow\n",
            "\u001b[2K    Found existing installation: pillow 11.2.1\n",
            "\u001b[2K    Uninstalling pillow-11.2.1:\n",
            "\u001b[2K      Successfully uninstalled pillow-11.2.1\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K  Attempting uninstall: markupsafe\n",
            "\u001b[2K    Found existing installation: MarkupSafe 3.0.2\n",
            "\u001b[2K    Uninstalling MarkupSafe-3.0.2:\n",
            "\u001b[2K      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[2K  Attempting uninstall: imageio-ffmpeg\n",
            "\u001b[2K    Found existing installation: imageio-ffmpeg 0.6.0\n",
            "\u001b[2K    Uninstalling imageio-ffmpeg-0.6.0:\n",
            "\u001b[2K      Successfully uninstalled imageio-ffmpeg-0.6.0\n",
            "\u001b[2K  Attempting uninstall: aiofiles\n",
            "\u001b[2K    Found existing installation: aiofiles 24.1.0\n",
            "\u001b[2K    Uninstalling aiofiles-24.1.0:\n",
            "\u001b[2K      Successfully uninstalled aiofiles-24.1.0\n",
            "\u001b[2K  Attempting uninstall: scipy\n",
            "\u001b[2K    Found existing installation: scipy 1.15.3\n",
            "\u001b[2K    Uninstalling scipy-1.15.3:\n",
            "\u001b[2K      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[2K  Attempting uninstall: rich\n",
            "\u001b[2K    Found existing installation: rich 13.9.4\n",
            "\u001b[2K    Uninstalling rich-13.9.4:\n",
            "\u001b[2K      Successfully uninstalled rich-13.9.4\n",
            "\u001b[2K  Attempting uninstall: opencv-python\n",
            "\u001b[2K    Found existing installation: opencv-python 4.11.0.86\n",
            "\u001b[2K    Uninstalling opencv-python-4.11.0.86:\n",
            "\u001b[2K      Successfully uninstalled opencv-python-4.11.0.86\n",
            "\u001b[2K  Attempting uninstall: imageio\n",
            "\u001b[2K    Found existing installation: imageio 2.37.0\n",
            "\u001b[2K    Uninstalling imageio-2.37.0:\n",
            "\u001b[2K      Successfully uninstalled imageio-2.37.0\n",
            "\u001b[2K  Attempting uninstall: tokenizers\n",
            "\u001b[2K    Found existing installation: tokenizers 0.21.1\n",
            "\u001b[2K    Uninstalling tokenizers-0.21.1:\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[2K  Attempting uninstall: scikit-image\n",
            "\u001b[2K    Found existing installation: scikit-image 0.25.2\n",
            "\u001b[2K    Uninstalling scikit-image-0.25.2:\n",
            "\u001b[2K      Successfully uninstalled scikit-image-0.25.2\n",
            "\u001b[2K  Attempting uninstall: matplotlib\n",
            "\u001b[2K    Found existing installation: matplotlib 3.10.0\n",
            "\u001b[2K    Uninstalling matplotlib-3.10.0:\n",
            "\u001b[2K      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[2K  Attempting uninstall: gradio-client\n",
            "\u001b[2K    Found existing installation: gradio_client 1.10.1\n",
            "\u001b[2K    Uninstalling gradio_client-1.10.1:\n",
            "\u001b[2K      Successfully uninstalled gradio_client-1.10.1\n",
            "\u001b[2K  Attempting uninstall: transformers\n",
            "\u001b[2K    Found existing installation: transformers 4.52.3\n",
            "\u001b[2K    Uninstalling transformers-4.52.3:\n",
            "\u001b[2K      Successfully uninstalled transformers-4.52.3\n",
            "\u001b[2K  Attempting uninstall: gradio\n",
            "\u001b[2K    Found existing installation: gradio 5.31.0\n",
            "\u001b[2K    Uninstalling gradio-5.31.0:\n",
            "\u001b[2K      Successfully uninstalled gradio-5.31.0\n",
            "\u001b[2K  Attempting uninstall: albumentations\n",
            "\u001b[2K    Found existing installation: albumentations 2.0.8\n",
            "\u001b[2K    Uninstalling albumentations-2.0.8:\n",
            "\u001b[2K      Successfully uninstalled albumentations-2.0.8\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [albumentations]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.61 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "google-genai 1.17.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\n",
            "dataproc-spark-connect 0.7.5 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\n",
            "dataproc-spark-connect 0.7.5 requires websockets>=14.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 albumentations-1.4.10 coloredlogs-15.0.1 ffmpeg-python-0.2.0 gradio-5.1.0 gradio-client-1.4.0 humanfriendly-10.0 imageio-2.34.2 imageio-ffmpeg-0.5.1 lmdb-1.4.1 markupsafe-2.1.5 matplotlib-3.9.0 numpy-1.26.4 onnx-1.16.1 onnxruntime-gpu-1.18.0 opencv-python-4.10.0.84 pillow-10.4.0 pykalman-0.9.7 pyyaml-6.0.1 rich-13.7.1 scikit-image-0.24.0 scipy-1.13.1 shtab-1.7.2 tokenizers-0.15.2 tomlkit-0.12.0 tqdm-4.66.4 transformers-4.38.0 tyro-0.8.5 websockets-12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              },
              "id": "52568449becb4108a9f9d37776ecbcac"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.41.0\n",
        "!pip install numpy==1.24.4 onnxruntime-gpu==1.17.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bFHkwX0gfmS-",
        "outputId": "8f6a34ea-64a0-4703-b7cb-2dac19a53960"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.41.0\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.0) (2025.4.26)\n",
            "Downloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "\u001b[2K  Attempting uninstall: tokenizers\n",
            "\u001b[2K    Found existing installation: tokenizers 0.15.2\n",
            "\u001b[2K    Uninstalling tokenizers-0.15.2:\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.15.2\n",
            "\u001b[2K  Attempting uninstall: transformers\n",
            "\u001b[2K    Found existing installation: transformers 4.38.0\n",
            "\u001b[2K    Uninstalling transformers-4.38.0:\n",
            "\u001b[2K      Successfully uninstalled transformers-4.38.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [transformers]\n",
            "\u001b[1A\u001b[2KSuccessfully installed tokenizers-0.19.1 transformers-4.41.0\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting onnxruntime-gpu==1.17.1\n",
            "  Downloading onnxruntime_gpu-1.17.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.17.1) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.17.1) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.17.1) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.17.1) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime-gpu==1.17.1) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime-gpu==1.17.1) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime-gpu==1.17.1) (1.3.0)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m140.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.17.1-cp311-cp311-manylinux_2_28_x86_64.whl (192.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.1/192.1 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, onnxruntime-gpu\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 1.26.4\n",
            "\u001b[2K    Uninstalling numpy-1.26.4:\n",
            "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[2K  Attempting uninstall: onnxruntime-gpu\n",
            "\u001b[2K    Found existing installation: onnxruntime-gpu 1.18.0\n",
            "\u001b[2K    Uninstalling onnxruntime-gpu-1.18.0:\n",
            "\u001b[2K      Successfully uninstalled onnxruntime-gpu-1.18.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [onnxruntime-gpu]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yfinance 0.2.61 requires websockets>=13.0, but you have websockets 12.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4 onnxruntime-gpu-1.17.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d9b81112f03a49039b31a81a97fc9022"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"PyTorch CUDA available? \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Current device:\", torch.cuda.get_device_name(0))\n",
        "    print(\"CUDA capability:\", torch.version.cuda)\n",
        "else:\n",
        "    print(\"No GPU detected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS3sB_yEge3v",
        "outputId": "6772f163-a278-4319-ecbf-e32f10555296"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CUDA available?  True\n",
            "Current device: Tesla T4\n",
            "CUDA capability: 12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download KwaiVGI/LivePortrait \\\n",
        "  --local-dir pretrained_weights \\\n",
        "  --exclude \"*.git*\" \"README.md\" \"docs\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kEk43yoggyG",
        "outputId": "a84be4b2-e52e-454a-f37b-21bd87ea87fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rFetching 20 files:   0% 0/20 [00:00<?, ?it/s]Downloading 'liveportrait/base_models/spade_generator.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait/base_models/7eKjApaVmkkjxwgjxli7XtNsnxU=.4780afc7909a9f84e24c01d73b31a555ef651521a1fe3b2429bd04534d992aee.incomplete'\n",
            "Downloading 'liveportrait/base_models/warping_module.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait/base_models/Kiu2mTGiqawQvwJIysiPiwdDHsY=.2f61a6f265fe344f14132364859a78bdbbc2068577170693da57fb96d636e282.incomplete'\n",
            "Downloading 'docs/showcase2.gif' to 'pretrained_weights/.cache/huggingface/download/docs/ovU00xd4wrM1WlqZO5_wVkCk_ec=.eb1fffb139681775780b2956e7d0289f55d199c1a3e14ab263887864d4b0d586.incomplete'\n",
            "Downloading 'liveportrait/base_models/motion_extractor.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait/base_models/K0D9rBQicolH41TOWUpuFUb_e4U=.251e6a94ad667a1d0c69526d292677165110ef7f0cf0f6d199f0e414e8aa0ca5.incomplete'\n",
            "Downloading 'insightface/models/buffalo_l/det_10g.onnx' to 'pretrained_weights/.cache/huggingface/download/insightface/models/buffalo_l/J2T53Etq4hZrLiuL9o-I5uPf3dU=.5838f7fe053675b1c7a08b633df49e7af5495cee0493c7dcf6697200b85b5b91.incomplete'\n",
            "\n",
            "\rspade_generator.pth:   0% 0.00/222M [00:00<?, ?B/s]\u001b[ADownloading 'liveportrait/base_models/appearance_feature_extractor.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait/base_models/w2JquOgGo_-zOZp03SpATaJOQmc=.5279bb8654293dbdf327030b397f107237dd9212fb11dd75b83dfb635211ceb5.incomplete'\n",
            "\n",
            "\n",
            "\rwarping_module.pth:   0% 0.00/182M [00:00<?, ?B/s]\u001b[A\u001b[ADownloading 'docs/inference.gif' to 'pretrained_weights/.cache/huggingface/download/docs/ke8qy6KU2hXc8UMbe9pzGl7iH4Q=.e1316eca5556ba5a8da7c53bcadbc1df26aa822bbde68fbad94813139803d0c6.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\rmotion_extractor.pth:   0% 0.00/113M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\rshowcase2.gif:   0% 0.00/2.88M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'insightface/models/buffalo_l/2d106det.onnx' to 'pretrained_weights/.cache/huggingface/download/insightface/models/buffalo_l/maPQ8fYWKyJky9lfTz9beisoSeo=.f001b856447c413801ef5c42091ed0cd516fcd21f2d6b79635b1e733a7109dbf.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\rdet_10g.onnx:   0% 0.00/16.9M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\rappearance_feature_extractor.pth:   0% 0.00/3.39M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\rinference.gif:   0% 0.00/820k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\r2d106det.onnx:   0% 0.00/5.03M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\rinference.gif: 100% 820k/820k [00:00<00:00, 21.4MB/s]\n",
            "Download complete. Moving file to pretrained_weights/docs/inference.gif\n",
            "Fetching 20 files:   5% 1/20 [00:00<00:04,  3.94it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "appearance_feature_extractor.pth: 100% 3.39M/3.39M [00:00<00:00, 30.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "2d106det.onnx: 100% 5.03M/5.03M [00:00<00:00, 37.2MB/s]\n",
            "Download complete. Moving file to pretrained_weights/insightface/models/buffalo_l/2d106det.onnx\n",
            "showcase2.gif: 100% 2.88M/2.88M [00:00<00:00, 17.3MB/s]\n",
            "Download complete. Moving file to pretrained_weights/docs/showcase2.gif\n",
            "appearance_feature_extractor.pth: 100% 3.39M/3.39M [00:00<00:00, 23.0MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait/base_models/appearance_feature_extractor.pth\n",
            "\n",
            "Fetching 20 files:  10% 2/20 [00:00<00:02,  6.08it/s]Downloading 'liveportrait/landmark.onnx' to 'pretrained_weights/.cache/huggingface/download/liveportrait/NFbC2M7-BGmftQYQOC0ieXTS16o=.31d22a5041326c31f19b78886939a634a5aedcaa5ab8b9b951a1167595d147db.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:   0% 0.00/115M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:   6% 10.5M/182M [00:00<00:03, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "det_10g.onnx:  62% 10.5M/16.9M [00:00<00:00, 46.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'liveportrait_animals/base_models/appearance_feature_extractor.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models/w2JquOgGo_-zOZp03SpATaJOQmc=.e2cd1d5d67c0457229e9736d401d39225e096895b869f34234978082561af6de.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:   9% 10.5M/113M [00:00<00:02, 37.9MB/s]\u001b[A\u001b[A\u001b[ADownloading 'liveportrait_animals/base_models/motion_extractor.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models/K0D9rBQicolH41TOWUpuFUb_e4U=.63c0d450099ef6ebece788ab711cb012509712e23fd1200b79fb65ef980adbb9.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "appearance_feature_extractor.pth:   0% 0.00/3.39M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'liveportrait/retargeting_models/stitching_retargeting_module.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait/retargeting_models/PNlJHjZewqMDsCN59DNOOoYQIJw=.3652d5a3f95099141a56986aaddec92fadf0a73c87a20fac9a2c07c32b28b611.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:   0% 0.00/113M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "stitching_retargeting_module.pth:   0% 0.00/2.39M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:   9% 21.0M/222M [00:00<00:03, 61.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "stitching_retargeting_module.pth: 100% 2.39M/2.39M [00:00<00:00, 31.7MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait/retargeting_models/stitching_retargeting_module.pth\n",
            "appearance_feature_extractor.pth: 100% 3.39M/3.39M [00:00<00:00, 31.2MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models/appearance_feature_extractor.pth\n",
            "\n",
            "\n",
            "warping_module.pth:  12% 21.0M/182M [00:00<00:02, 53.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "det_10g.onnx: 100% 16.9M/16.9M [00:00<00:00, 41.6MB/s]\n",
            "Download complete. Moving file to pretrained_weights/insightface/models/buffalo_l/det_10g.onnx\n",
            "Fetching 20 files:  20% 4/20 [00:00<00:02,  6.96it/s]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:   9% 10.5M/113M [00:00<00:01, 64.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  14% 31.5M/222M [00:00<00:02, 66.2MB/s]\u001b[ADownloading 'liveportrait_animals/base_models/warping_module.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models/Kiu2mTGiqawQvwJIysiPiwdDHsY=.c9719ea184ca9da059f4eee8a8c8c7c6bd46a2b1e40a241ea5490cc42ce6b79b.incomplete'\n",
            "Downloading 'liveportrait_animals/base_models/spade_generator.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models/7eKjApaVmkkjxwgjxli7XtNsnxU=.7fafa1e31c7c72c9384310d679e32af3fbf214e241fb657df8c3b18ad826f336.incomplete'\n",
            "\n",
            "\n",
            "warping_module.pth:  17% 31.5M/182M [00:00<00:02, 61.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:   0% 0.00/182M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:   0% 0.00/222M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  19% 21.0M/113M [00:00<00:02, 36.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  19% 21.0M/113M [00:00<00:01, 76.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  18% 21.0M/115M [00:00<00:01, 51.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'liveportrait_animals/base_models_v1.1/appearance_feature_extractor.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models_v1.1/w2JquOgGo_-zOZp03SpATaJOQmc=.7e320d545579caa83c7b094cef8b7b43fe92a2e410c219ffa97b08be549f45bf.incomplete'\n",
            "\n",
            "spade_generator.pth:  19% 41.9M/222M [00:00<00:02, 62.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "appearance_feature_extractor.pth:   0% 0.00/3.39M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  23% 41.9M/182M [00:00<00:02, 60.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:   6% 10.5M/182M [00:00<00:02, 62.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "appearance_feature_extractor.pth: 100% 3.39M/3.39M [00:00<00:00, 38.8MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models_v1.1/appearance_feature_extractor.pth\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  28% 31.5M/113M [00:00<00:02, 40.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  28% 31.5M/113M [00:00<00:01, 61.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  27% 31.5M/115M [00:00<00:01, 50.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  24% 52.4M/222M [00:00<00:02, 61.1MB/s]\u001b[ADownloading 'liveportrait_animals/base_models_v1.1/motion_extractor.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models_v1.1/K0D9rBQicolH41TOWUpuFUb_e4U=.827d0ea4c56ff252dba50feece3bc62ced365ecae5edb86db07eb71b2f39a696.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:   0% 0.00/113M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  29% 52.4M/182M [00:00<00:02, 57.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  37% 41.9M/113M [00:00<00:01, 62.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  37% 41.9M/115M [00:00<00:01, 51.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  12% 21.0M/182M [00:00<00:03, 40.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  37% 41.9M/113M [00:01<00:01, 39.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  28% 62.9M/222M [00:01<00:02, 55.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:   9% 21.0M/222M [00:00<00:05, 36.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:   9% 10.5M/113M [00:00<00:02, 48.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  35% 62.9M/182M [00:01<00:02, 54.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  47% 52.4M/113M [00:01<00:03, 18.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  17% 31.5M/182M [00:01<00:10, 14.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  14% 31.5M/222M [00:01<00:13, 14.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  46% 52.4M/115M [00:02<00:03, 17.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  33% 73.4M/222M [00:02<00:08, 18.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  47% 52.4M/113M [00:02<00:03, 15.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  19% 21.0M/113M [00:01<00:07, 11.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  40% 73.4M/182M [00:02<00:05, 18.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  23% 41.9M/182M [00:01<00:06, 21.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  56% 62.9M/113M [00:02<00:02, 21.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  55% 62.9M/115M [00:02<00:02, 21.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  19% 41.9M/222M [00:02<00:09, 19.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  46% 83.9M/182M [00:02<00:04, 23.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  56% 62.9M/113M [00:02<00:02, 20.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  29% 52.4M/182M [00:02<00:05, 25.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  28% 31.5M/113M [00:01<00:04, 17.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  38% 83.9M/222M [00:02<00:06, 21.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  65% 73.4M/113M [00:02<00:01, 24.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  24% 52.4M/222M [00:02<00:07, 22.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  64% 73.4M/115M [00:02<00:01, 23.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  52% 94.4M/182M [00:02<00:03, 26.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  35% 62.9M/182M [00:02<00:04, 28.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  65% 73.4M/113M [00:03<00:01, 22.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  37% 41.9M/113M [00:02<00:03, 21.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  43% 94.4M/222M [00:03<00:05, 23.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  75% 83.9M/113M [00:02<00:00, 28.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  73% 83.9M/115M [00:02<00:01, 28.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  58% 105M/182M [00:03<00:02, 30.6MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  28% 62.9M/222M [00:02<00:05, 27.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  33% 73.4M/222M [00:03<00:09, 15.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  40% 73.4M/182M [00:03<00:07, 14.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  82% 94.4M/115M [00:04<00:01, 15.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  47% 105M/222M [00:04<00:08, 14.3MB/s] \u001b[A\n",
            "\n",
            "warping_module.pth:  63% 115M/182M [00:04<00:04, 16.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  75% 83.9M/113M [00:04<00:02, 13.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  84% 94.4M/113M [00:04<00:01, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  47% 52.4M/113M [00:03<00:04, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  38% 83.9M/222M [00:04<00:06, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  46% 83.9M/182M [00:04<00:05, 19.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx:  91% 105M/115M [00:04<00:00, 20.7MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  52% 115M/222M [00:04<00:05, 18.8MB/s]\u001b[A\n",
            "\n",
            "warping_module.pth:  69% 126M/182M [00:04<00:02, 20.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  84% 94.4M/113M [00:04<00:01, 17.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  93% 105M/113M [00:04<00:00, 18.6MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  56% 62.9M/113M [00:03<00:03, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  43% 94.4M/222M [00:04<00:05, 24.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  52% 94.4M/182M [00:04<00:03, 23.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  57% 126M/222M [00:04<00:04, 22.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "landmark.onnx: 100% 115M/115M [00:04<00:00, 24.1MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait/landmark.onnx\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth: 100% 113M/113M [00:04<00:00, 21.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "motion_extractor.pth: 100% 113M/113M [00:04<00:00, 23.9MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models/motion_extractor.pth\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  65% 73.4M/113M [00:04<00:01, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "motion_extractor.pth:  93% 105M/113M [00:05<00:00, 20.4MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  47% 105M/222M [00:04<00:04, 28.8MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  61% 136M/222M [00:05<00:03, 27.3MB/s]\u001b[ADownloading 'liveportrait_animals/base_models_v1.1/spade_generator.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models_v1.1/7eKjApaVmkkjxwgjxli7XtNsnxU=.6ac31a9b608f3920ec41a402b1c4e29d22007dafd79a855f204aae9307039445.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  58% 105M/182M [00:04<00:02, 27.4MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:   0% 0.00/222M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  75% 83.9M/113M [00:04<00:01, 26.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading 'liveportrait_animals/base_models_v1.1/warping_module.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/base_models_v1.1/Kiu2mTGiqawQvwJIysiPiwdDHsY=.7dfd251dc6b3a1baefebfc658f5bb2a2c565649cdb9aa75032591e824a0bfcee.incomplete'\n",
            "motion_extractor.pth: 100% 113M/113M [00:05<00:00, 24.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  81% 147M/182M [00:05<00:01, 28.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  52% 115M/222M [00:04<00:03, 35.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth: 100% 113M/113M [00:05<00:00, 21.4MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait/base_models/motion_extractor.pth\n",
            "Fetching 20 files:  30% 6/20 [00:05<00:16,  1.21s/it]\n",
            "spade_generator.pth:  66% 147M/222M [00:05<00:02, 31.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:   5% 10.5M/222M [00:00<00:03, 55.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  84% 94.4M/113M [00:04<00:00, 32.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  63% 115M/182M [00:04<00:02, 30.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[ADownloading 'liveportrait_animals/retargeting_models/stitching_retargeting_module.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/retargeting_models/PNlJHjZewqMDsCN59DNOOoYQIJw=.3652d5a3f95099141a56986aaddec92fadf0a73c87a20fac9a2c07c32b28b611.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  57% 126M/222M [00:04<00:02, 39.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  86% 157M/182M [00:05<00:00, 32.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:   6% 10.5M/182M [00:00<00:03, 45.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "stitching_retargeting_module.pth:   0% 0.00/2.39M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "stitching_retargeting_module.pth: 100% 2.39M/2.39M [00:00<00:00, 31.6MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/retargeting_models/stitching_retargeting_module.pth\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  69% 126M/182M [00:05<00:01, 34.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth:  93% 105M/113M [00:04<00:00, 34.9MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  76% 168M/222M [00:05<00:01, 42.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  61% 136M/222M [00:05<00:02, 42.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "warping_module.pth:  92% 168M/182M [00:05<00:00, 35.7MB/s]\u001b[A\u001b[ADownloading 'liveportrait_animals/xpose.pth' to 'pretrained_weights/.cache/huggingface/download/liveportrait_animals/VjOeoPidTrEBdSIS2lTXTeOx6ys=.bf58e5a3c4a3a017198edc69e33f89c9a37adc856fe6b1776059b2d4a524a7dd.incomplete'\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  14% 31.5M/222M [00:00<00:03, 61.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  12% 21.0M/182M [00:00<00:03, 48.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:   0% 0.00/435M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  75% 136M/182M [00:05<00:01, 40.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  80% 178M/222M [00:05<00:00, 47.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "motion_extractor.pth: 100% 113M/113M [00:04<00:00, 36.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  66% 147M/222M [00:05<00:01, 43.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "motion_extractor.pth: 100% 113M/113M [00:05<00:00, 21.9MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models_v1.1/motion_extractor.pth\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  19% 41.9M/222M [00:00<00:04, 39.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:   5% 21.0M/435M [00:00<00:07, 52.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  81% 147M/182M [00:05<00:01, 35.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  71% 157M/222M [00:05<00:01, 41.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  17% 31.5M/182M [00:00<00:04, 32.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "warping_module.pth: 100% 182M/182M [00:06<00:00, 29.3MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait/base_models/warping_module.pth\n",
            "\n",
            "\n",
            "\n",
            "xpose.pth:   7% 31.5M/435M [00:00<00:06, 58.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  24% 52.4M/222M [00:01<00:04, 41.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  23% 41.9M/182M [00:01<00:03, 38.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  90% 199M/222M [00:06<00:00, 39.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  86% 157M/182M [00:05<00:00, 34.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  76% 168M/222M [00:05<00:01, 38.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  10% 41.9M/435M [00:00<00:07, 54.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  29% 52.4M/182M [00:03<00:10, 11.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  28% 62.9M/222M [00:03<00:12, 12.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  95% 210M/222M [00:08<00:00, 14.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  35% 62.9M/182M [00:03<00:08, 14.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  80% 178M/222M [00:08<00:03, 12.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  92% 168M/182M [00:08<00:01, 11.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  33% 73.4M/222M [00:03<00:09, 15.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "spade_generator.pth:  99% 220M/222M [00:08<00:00, 16.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "spade_generator.pth: 100% 222M/222M [00:08<00:00, 25.3MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait/base_models/spade_generator.pth\n",
            "Fetching 20 files:  35% 7/20 [00:08<00:23,  1.79s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  40% 73.4M/182M [00:03<00:05, 20.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  38% 83.9M/222M [00:03<00:06, 20.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  98% 178M/182M [00:08<00:00, 15.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  85% 189M/222M [00:08<00:02, 16.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "warping_module.pth: 100% 182M/182M [00:08<00:00, 21.8MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models/warping_module.pth\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  46% 83.9M/182M [00:03<00:03, 26.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  43% 94.4M/222M [00:03<00:04, 26.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  90% 199M/222M [00:08<00:01, 20.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  17% 73.4M/435M [00:03<00:17, 21.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  47% 105M/222M [00:03<00:03, 33.9MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  52% 94.4M/182M [00:03<00:02, 32.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  95% 210M/222M [00:08<00:00, 27.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  58% 105M/182M [00:04<00:03, 21.9MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  99% 220M/222M [00:09<00:00, 20.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  22% 94.4M/435M [00:04<00:15, 21.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  57% 126M/222M [00:04<00:03, 27.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  63% 115M/182M [00:04<00:02, 27.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  24% 105M/435M [00:04<00:12, 26.8MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth: 100% 222M/222M [00:09<00:00, 23.2MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models/spade_generator.pth\n",
            "Fetching 20 files:  65% 13/20 [00:10<00:05,  1.39it/s]\n",
            "\n",
            "\n",
            "xpose.pth:  27% 115M/435M [00:04<00:09, 32.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  66% 147M/222M [00:05<00:01, 38.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  69% 126M/182M [00:05<00:01, 30.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  29% 126M/435M [00:04<00:08, 38.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  71% 157M/222M [00:05<00:01, 44.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  34% 147M/435M [00:04<00:05, 51.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  81% 147M/182M [00:05<00:00, 41.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  80% 178M/222M [00:05<00:00, 55.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  86% 157M/182M [00:05<00:00, 46.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  85% 189M/222M [00:05<00:00, 55.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  39% 168M/435M [00:05<00:04, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  90% 199M/222M [00:05<00:00, 59.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  41% 178M/435M [00:05<00:04, 62.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth:  92% 168M/182M [00:05<00:00, 38.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  95% 210M/222M [00:06<00:00, 57.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  43% 189M/435M [00:05<00:04, 59.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "warping_module.pth: 100% 182M/182M [00:06<00:00, 44.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "spade_generator.pth:  99% 220M/222M [00:06<00:00, 54.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "warping_module.pth: 100% 182M/182M [00:06<00:00, 29.4MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models_v1.1/warping_module.pth\n",
            "spade_generator.pth: 100% 222M/222M [00:06<00:00, 34.6MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/base_models_v1.1/spade_generator.pth\n",
            "Fetching 20 files:  85% 17/20 [00:11<00:01,  1.73it/s]\n",
            "\n",
            "\n",
            "xpose.pth:  48% 210M/435M [00:05<00:03, 59.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  51% 220M/435M [00:06<00:03, 59.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  53% 231M/435M [00:06<00:03, 63.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  55% 241M/435M [00:06<00:03, 63.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  58% 252M/435M [00:06<00:02, 66.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  60% 262M/435M [00:06<00:02, 67.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  63% 273M/435M [00:06<00:02, 69.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  65% 283M/435M [00:06<00:02, 66.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  72% 315M/435M [00:07<00:01, 115MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  80% 346M/435M [00:07<00:00, 158MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  87% 377M/435M [00:07<00:00, 179MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth:  94% 409M/435M [00:07<00:00, 198MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "xpose.pth: 100% 435M/435M [00:07<00:00, 57.6MB/s]\n",
            "Download complete. Moving file to pretrained_weights/liveportrait_animals/xpose.pth\n",
            "Fetching 20 files: 100% 20/20 [00:13<00:00,  1.49it/s]\n",
            "/content/LivePortrait/pretrained_weights\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "_ = torch.zeros((1,1), device=\"cuda\")\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Vhj0lmANgpaT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=memory.used --format=csv,nounits,noheader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0pCFIwugs9W",
        "outputId": "b7320c79-d21c-4d49-ca22-38091079f55f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "SRC_IMG = \"/content/LivePortrait/assets/examples/source/s9.jpg\"\n",
        "DRIVE_VID = \"/content/LivePortrait/assets/examples/driving/d0.mp4\"\n",
        "OUT_DIR = \"animations\"\n",
        "\n",
        "start_time = time.time()\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        [\n",
        "            sys.executable, \"/content/LivePortrait/inference.py\",\n",
        "            \"-s\", SRC_IMG,\n",
        "            \"-d\", DRIVE_VID,\n",
        "            \"-o\", OUT_DIR\n",
        "        ],\n",
        "        check=True,\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\" inference.py exited with a non-zero status.\")\n",
        "    print(\"------ stdout ------\")\n",
        "    print(e.stdout)\n",
        "    print(\"------ stderr ------\")\n",
        "    print(e.stderr)\n",
        "    raise\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_ms = (end_time - start_time) * 1000\n",
        "print(f\"=== Elapsed inference time (FP32): {elapsed_ms:.0f} ms total ===\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU-cF_cVgvpU",
        "outputId": "fcc29461-3f4e-4a1a-f2cd-6d86fd01d107"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Elapsed inference time (FP32): 47294 ms total ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from src.modules.spade_generator import SPADEDecoder  # Adjust this path if needed\n",
        "\n",
        "generator = SPADEDecoder()\n",
        "generator.eval().cuda()\n",
        "\n",
        "ckpt_path = \"/content/LivePortrait/pretrained_weights/liveportrait/base_models/spade_generator.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location=\"cuda\")\n",
        "\n",
        "generator.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "dummy_input = torch.randn(512, 256, 3, 3).cuda()\n",
        "with torch.cuda.amp.autocast(), torch.no_grad():\n",
        "    output = generator(dummy_input)\n",
        "\n",
        "print(\" Inference complete. Output shape:\", output.shape)\n",
        "\n",
        "traced = torch.jit.trace(generator, dummy_input)\n",
        "torchscript_path = \"/content/spade_generator_torchscript.pt\"\n",
        "traced.save(torchscript_path)\n",
        "print(f\" TorchScript model saved at: {torchscript_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Jm0uDvKgyGl",
        "outputId": "b4da0b49-08e8-497f-9185-e8aa0cea07a9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference complete. Output shape: torch.Size([512, 3, 12, 12])\n",
            " TorchScript model saved at: /content/spade_generator_torchscript.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    _ = generator(dummy_input)\n",
        "\n",
        "# Time it\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = generator(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Original Model Avg Inference Time: {(end - start)/100:.6f} sec\")\n",
        "optimized_model = torch.jit.load(\"/content/spade_generator_torchscript.pt\").eval().cuda()\n",
        "\n",
        "# Warm-up\n",
        "for _ in range(10):\n",
        "    _ = optimized_model(dummy_input)\n",
        "\n",
        "# Time it\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = optimized_model(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"TorchScript Model Avg Inference Time: {(end - start)/100:.6f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ8lMjTNhAP0",
        "outputId": "41d26361-a79d-4ba5-9bff-47a87641762c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Avg Inference Time: 0.364458 sec\n",
            "TorchScript Model Avg Inference Time: 0.331911 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from src.modules.motion_extractor import MotionExtractor\n",
        "\n",
        "generator = MotionExtractor(num_kp=21)\n",
        "generator.eval().cuda()\n",
        "\n",
        "ckpt_path = \"/content/LivePortrait/pretrained_weights/liveportrait/base_models/motion_extractor.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location=\"cuda\")\n",
        "generator.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "dummy_input = torch.randn(1,3,256,256).cuda()\n",
        "\n",
        "with torch.cuda.amp.autocast(), torch.no_grad():\n",
        "    output = dict(generator(dummy_input))\n",
        "\n",
        "print(\" Inference complete. Output shape:\", output)\n",
        "\n",
        "\n",
        "traced = torch.jit.trace(generator, dummy_input, strict=False)\n",
        "torchscript_path = \"/content/motion_extractor_torchscript.pt\"\n",
        "traced.save(torchscript_path)\n",
        "print(f\" TorchScript model saved at: {torchscript_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8jqQHh9hD3F",
        "outputId": "c1ab6328-8d4e-45ff-aa11-e5519c181355"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inference complete. Output shape: {'pitch': tensor([[ 1.3418,  1.3535,  1.3408,  1.2930,  1.2197,  1.1113,  0.9795,  0.8403,\n",
            "          0.6836,  0.5117,  0.3328,  0.1509, -0.0259, -0.1992, -0.3647, -0.5127,\n",
            "         -0.6436, -0.7510, -0.8281, -0.8672, -0.8652, -0.7993, -0.6763, -0.4839,\n",
            "         -0.2725, -0.1049, -0.1070, -0.4238, -1.0283, -1.7207, -2.2930, -2.5840,\n",
            "         -2.4121, -3.5781, -3.5215, -3.2383, -2.4141, -1.6260, -1.1973, -1.0576,\n",
            "         -1.0459, -1.1006, -1.1885, -1.2588, -1.2939, -1.2881, -1.2412, -1.1631,\n",
            "         -1.0635, -0.9541, -0.8379, -0.7188, -0.5996, -0.4822, -0.3679, -0.2576,\n",
            "         -0.1501, -0.0508,  0.0456,  0.1295,  0.2047,  0.2712,  0.3335,  0.3904,\n",
            "          0.4426,  0.4905]], device='cuda:0', dtype=torch.float16), 'yaw': tensor([[-3.9883, -2.9707, -1.9854, -1.0771, -0.2788,  0.4021,  0.9668,  1.4219,\n",
            "          1.7754,  2.0332,  2.2031,  2.2930,  2.3086,  2.2578,  2.1602,  2.0352,\n",
            "          1.8838,  1.7148,  1.5293,  1.3281,  1.1123,  0.8779,  0.6245,  0.3491,\n",
            "          0.0512, -0.2666, -0.5942, -0.9102, -1.1299, -1.0820, -1.3652, -4.9922,\n",
            "         -3.1406, -2.1719, -2.5176, -2.7676, -1.6689, -1.0283, -0.4338,  0.0814,\n",
            "          0.4832,  0.7969,  1.0449,  1.2412,  1.3994,  1.5293,  1.6338,  1.7188,\n",
            "          1.7920,  1.8545,  1.9072,  1.9502,  1.9863,  2.0156,  2.0293,  2.0156,\n",
            "          1.9229,  1.7002,  1.3340,  0.8511,  0.2629, -0.4209, -1.1934, -2.0215,\n",
            "         -2.8730, -3.7012]], device='cuda:0', dtype=torch.float16), 'roll': tensor([[ 0.0739,  0.1672,  0.2296,  0.2754,  0.3076,  0.3413,  0.3811,  0.4155,\n",
            "          0.4392,  0.4302,  0.4109,  0.3330,  0.2137,  0.0304, -0.1997, -0.3977,\n",
            "         -0.6387, -0.8379, -1.0332, -1.1426, -1.2324, -1.2754, -1.2305, -1.1396,\n",
            "         -0.9780, -0.7061, -0.3572,  0.1367,  0.7319,  1.3037,  1.4883,  0.1006,\n",
            "         -4.0664, -2.1836, -0.7300, -1.9150, -1.4004, -0.9033, -0.5620, -0.3879,\n",
            "         -0.3113, -0.2861, -0.2844, -0.3005, -0.3286, -0.3635, -0.4109, -0.4675,\n",
            "         -0.5156, -0.5586, -0.5547, -0.5308, -0.4639, -0.3047, -0.1149,  0.1240,\n",
            "          0.3440,  0.5210,  0.6445,  0.7056,  0.7041,  0.6279,  0.5073,  0.3223,\n",
            "          0.1116, -0.1226]], device='cuda:0', dtype=torch.float16), 't': tensor([[0.2466, 0.0472, 0.0000]], device='cuda:0', dtype=torch.float16), 'exp': tensor([[-1.7700e-01, -5.0488e-01, -1.0786e-03,  6.3110e-02, -9.5312e-01,\n",
            "         -2.3186e-04, -1.8030e-01, -6.5771e-01, -8.8692e-04,  3.6523e-01,\n",
            "          6.2061e-01,  5.1270e-03, -4.1968e-01,  2.4817e-01, -3.3245e-03,\n",
            "         -2.4280e-01,  4.1138e-01,  8.4782e-04,  5.1660e-01,  7.4951e-01,\n",
            "          8.9765e-05, -5.0879e-01,  4.6948e-01,  3.5524e-04,  4.6460e-01,\n",
            "         -1.0999e-01, -7.2098e-04, -4.3262e-01, -4.0332e-01,  5.1856e-05,\n",
            "         -1.2610e-01,  1.1517e-01, -9.4299e-02,  7.8087e-03, -4.4464e-02,\n",
            "         -7.7534e-04,  1.4197e-01, -1.9788e-01, -5.2166e-04,  2.0398e-01,\n",
            "         -7.8003e-02, -6.0081e-04, -2.9980e-01, -2.6904e-01, -5.0449e-04,\n",
            "         -3.0542e-01, -1.3206e-02, -7.6151e-04, -2.2510e-01, -1.6028e-01,\n",
            "         -5.1200e-05,  1.9873e-01,  4.8511e-01, -1.4905e-01, -1.6101e-01,\n",
            "          6.4844e-01, -1.5259e-03, -6.5918e-02,  4.2572e-02, -1.1005e-01,\n",
            "          1.8295e-02,  2.3486e-01, -6.0089e-02]], device='cuda:0',\n",
            "       dtype=torch.float16), 'scale': tensor([[0.8818]], device='cuda:0', dtype=torch.float16), 'kp': tensor([[ 0.2162, -0.4895,  0.1736, -0.4272, -0.4812, -0.1606,  0.4331, -0.4238,\n",
            "         -0.1898, -0.4009,  0.2313,  0.0847,  0.3914,  0.2837,  0.1962,  0.1260,\n",
            "          0.3105,  0.2664, -0.9854,  0.9624,  0.1915,  0.4482,  0.4270,  0.0277,\n",
            "         -0.4617, -0.0734, -0.0380,  0.5010,  0.0775,  0.0066, -0.1560, -0.0947,\n",
            "          0.1700, -0.3167, -0.0624, -0.0335, -0.2610,  0.2438, -0.1311, -0.1769,\n",
            "          0.0294, -0.0907,  0.4360, -0.0422, -0.1913,  0.2201, -0.0035, -0.1220,\n",
            "          0.3057, -0.0832, -0.0124,  0.0209,  0.1959, -0.1688,  0.0112, -0.0911,\n",
            "          0.0093, -0.0743,  0.2856, -0.2225, -0.1292,  0.1475, -0.2192]],\n",
            "       device='cuda:0', dtype=torch.float16)}\n",
            " TorchScript model saved at: /content/motion_extractor_torchscript.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    _ = generator(dummy_input)\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = generator(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Original Model Avg Inference Time: {(end - start)/100:.6f} sec\")\n",
        "optimized_model = torch.jit.load(\"/content/motion_extractor_torchscript.pt\").eval().cuda()\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = optimized_model(dummy_input)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = optimized_model(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"TorchScript Model Avg Inference Time: {(end - start)/100:.6f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ4TqY8khHtZ",
        "outputId": "64461c30-7b80-44d4-fdd7-22356cc4b5d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Avg Inference Time: 0.011086 sec\n",
            "TorchScript Model Avg Inference Time: 0.007991 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from src.modules.appearance_feature_extractor import AppearanceFeatureExtractor\n",
        "\n",
        "\n",
        "model = AppearanceFeatureExtractor(\n",
        "    image_channel=3,\n",
        "    block_expansion=64,\n",
        "    num_down_blocks=2,\n",
        "    max_features=512,\n",
        "    reshape_channel=32,\n",
        "    reshape_depth=16,\n",
        "    num_resblocks=6\n",
        ")\n",
        "model.eval().cuda()\n",
        "\n",
        "\n",
        "ckpt_path = \"/content/LivePortrait/pretrained_weights/liveportrait/base_models/appearance_feature_extractor.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location=\"cuda\")\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 256, 256).cuda()\n",
        "\n",
        "\n",
        "traced = torch.jit.trace(model, dummy_input, strict=False)\n",
        "\n",
        "\n",
        "torchscript_path = \"/content/appearance_feature_extractor.pt\"\n",
        "traced.save(torchscript_path)\n",
        "\n",
        "print(f\" TorchScript model saved at: {torchscript_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNqvtkgchZLd",
        "outputId": "4d52e7f2-6329-4171-fabc-d91aa796ebbe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " TorchScript model saved at: /content/appearance_feature_extractor.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(10):\n",
        "    _ = generator(dummy_input)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = generator(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Original Model Avg Inference Time: {(end - start)/100:.6f} sec\")\n",
        "optimized_model = torch.jit.load(\"/content/appearance_feature_extractor.pt\").eval().cuda()\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = optimized_model(dummy_input)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = optimized_model(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"TorchScript Model Avg Inference Time: {(end - start)/100:.6f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS0ckEHehdbE",
        "outputId": "78f74b85-513c-4b11-f58b-ec37e8bc5749"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Avg Inference Time: 0.011196 sec\n",
            "TorchScript Model Avg Inference Time: 0.024077 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from src.modules.stitching_retargeting_network import StitchingRetargetingNetwork  # Adjust this path if needed\n",
        "\n",
        "\n",
        "generator = StitchingRetargetingNetwork( input_size=256,\n",
        "    hidden_sizes=[512, 512, 256],\n",
        "    output_size=3)\n",
        "generator.eval().cuda()\n",
        "\n",
        "\n",
        "ckpt_path = \"/content/LivePortrait/pretrained_weights/liveportrait/retargeting_models/stitching_retargeting_module.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location=\"cuda\")\n",
        "for name, param in generator.named_parameters():\n",
        "    print(name, param.shape)\n",
        "\n",
        "generator.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "\n",
        "dummy_input = torch.randn(1, 256).cuda()\n",
        "with torch.cuda.amp.autocast(), torch.no_grad():\n",
        "    output = generator(dummy_input)\n",
        "\n",
        "print(\" Inference complete. Output shape:\", output.shape)\n",
        "\n",
        "\n",
        "traced = torch.jit.trace(generator, dummy_input)\n",
        "torchscript_path = \"/content/stitching_retargeting_torchscript.pt\"\n",
        "traced.save(torchscript_path)\n",
        "print(f\" TorchScript model saved at: {torchscript_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5kG_v8QhemQ",
        "outputId": "364b0c99-cc5d-4303-84d9-be49c6049c3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mlp.0.weight torch.Size([512, 256])\n",
            "mlp.0.bias torch.Size([512])\n",
            "mlp.2.weight torch.Size([512, 512])\n",
            "mlp.2.bias torch.Size([512])\n",
            "mlp.4.weight torch.Size([256, 512])\n",
            "mlp.4.bias torch.Size([256])\n",
            "mlp.6.weight torch.Size([3, 256])\n",
            "mlp.6.bias torch.Size([3])\n",
            " Inference complete. Output shape: torch.Size([1, 3])\n",
            " TorchScript model saved at: /content/stitching_retargeting_torchscript.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import time\n",
        "from src.modules.stitching_retargeting_network import StitchingRetargetingNetwork  # Adjust this path if needed\n",
        "\n",
        "\n",
        "generator = StitchingRetargetingNetwork( input_size=256,\n",
        "    hidden_sizes=[512, 512, 256],\n",
        "    output_size=3)\n",
        "ckpt_path = \"/content/LivePortrait/pretrained_weights/liveportrait/retargeting_models/stitching_retargeting_module.pth\"\n",
        "state_dict = torch.load(ckpt_path, map_location=\"cuda\")\n",
        "generator.load_state_dict(state_dict, strict=False)\n",
        "generator.eval().cuda()\n",
        "dummy_input = torch.randn(1, 256).cuda()\n",
        "for _ in range(10):\n",
        "    _ = generator(dummy_input)\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = generator(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"Original Model Avg Inference Time: {(end - start)/100:.6f} sec\")\n",
        "optimized_model = torch.jit.load(\"/content/stitching_retargeting_torchscript.pt\").eval().cuda()\n",
        "\n",
        "\n",
        "for _ in range(10):\n",
        "    _ = optimized_model(dummy_input)\n",
        "\n",
        "start = time.time()\n",
        "for _ in range(100):\n",
        "    _ = optimized_model(dummy_input)\n",
        "end = time.time()\n",
        "\n",
        "print(f\"TorchScript Model Avg Inference Time: {(end - start)/100:.6f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihNchqKMhgvo",
        "outputId": "f7ff8951-b29d-449a-b8e0-b0b1c0aeee02"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Avg Inference Time: 0.000181 sec\n",
            "TorchScript Model Avg Inference Time: 0.000131 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import torch\n",
        "from src.config.inference_config import InferenceConfig\n",
        "from src.config.crop_config import CropConfig\n",
        "from src.live_portrait_pipeline import LivePortraitPipeline\n",
        "from IPython.display import Video, display\n",
        "from src.config.argument_config import ArgumentConfig\n",
        "\n",
        "SRC_IMG = \"/content/LivePortrait/assets/examples/source/s9.jpg\"\n",
        "DRIVE_VID = \"/content/LivePortrait/assets/examples/driving/d0.mp4\"\n",
        "OUT_DIR = \"animations\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "parsed_args = ArgumentConfig(source=SRC_IMG, driving=DRIVE_VID, output_dir=OUT_DIR)\n",
        "\n",
        "pipeline = LivePortraitPipeline(inference_cfg=InferenceConfig(), crop_cfg=CropConfig())\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.cuda.amp.autocast(), torch.no_grad():\n",
        "    animation = pipeline.execute(parsed_args)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "elapsed_ms = (end_time - start_time) * 1000\n",
        "print(f\"=== Elapsed inference time (AMP only): {elapsed_ms:.0f} ms total ===\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565,
          "referenced_widgets": [
            "25061151c9a7419dbcf28b25f5f917b0",
            "3c8aad139c8c4a698e981bbfc2e27858",
            "37a72a5a651b472ca1a132146e3d625b",
            "a2bbd48fecbe49f8822c79ad3b0cd141",
            "4ad6c5160bcd49e08138966b7f9f1530",
            "6fb07c397b4e4d76ab7e5004261842d9",
            "87c69c0018b143c88ea80da0ed25a862",
            "b23f94065e9c40ce87aa67c8a04a6fc2",
            "9aa42d3e610e4053a9c7f5b4c82ce118",
            "4e9adadc45c34b1497334864b5fd3a8e"
          ]
        },
        "id": "tbYpIKVVhjj1",
        "outputId": "5dc9f57a-edb1-43bc-ed25-015a290ae10d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:19:37]\u001b[0m\u001b[2;36m \u001b[0mLoad appearance_feature_extractor from                                       \u001b]8;id=61433;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=755043;file:///content/LivePortrait/src/live_portrait_wrapper.py#46\u001b\\\u001b[2m46\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mappearance\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95m_feature_extractor.pth\u001b[0m done.                                                 \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:19:37] </span>Load appearance_feature_extractor from                                       <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#46\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">46</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">appearance</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">_feature_extractor.pth</span> done.                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad motion_extractor from                                                   \u001b]8;id=455017;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=522845;file:///content/LivePortrait/src/live_portrait_wrapper.py#49\u001b\\\u001b[2m49\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mmotion_ext\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mractor.pth\u001b[0m done.                                                             \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Load motion_extractor from                                                   <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#49\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">49</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">motion_ext</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">ractor.pth</span> done.                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:19:38]\u001b[0m\u001b[2;36m \u001b[0mLoad warping_module from                                                     \u001b]8;id=351275;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=843950;file:///content/LivePortrait/src/live_portrait_wrapper.py#52\u001b\\\u001b[2m52\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mwarping_mo\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mdule.pth\u001b[0m done.                                                               \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:19:38] </span>Load warping_module from                                                     <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#52\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">warping_mo</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">dule.pth</span> done.                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:19:39]\u001b[0m\u001b[2;36m \u001b[0mLoad spade_generator from                                                    \u001b]8;id=206954;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=186778;file:///content/LivePortrait/src/live_portrait_wrapper.py#55\u001b\\\u001b[2m55\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/base_models/\u001b[0m\u001b[95mspade_gene\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mrator.pth\u001b[0m done.                                                              \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:19:39] </span>Load spade_generator from                                                    <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#55\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/base_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">spade_gene</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">rator.pth</span> done.                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad stitching_retargeting_module from                                       \u001b]8;id=245280;file:///content/LivePortrait/src/live_portrait_wrapper.py\u001b\\\u001b[2mlive_portrait_wrapper.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=964697;file:///content/LivePortrait/src/live_portrait_wrapper.py#59\u001b\\\u001b[2m59\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/pretrained_weights/liveportrait/retargeting_models/\u001b[0m\u001b[95msti\u001b[0m \u001b[2m                           \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[95mtching_retargeting_module.pth\u001b[0m done.                                          \u001b[2m                           \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Load stitching_retargeting_module from                                       <a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_wrapper.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_wrapper.py#59\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">59</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/pretrained_weights/liveportrait/retargeting_models/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">sti</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">tching_retargeting_module.pth</span> done.                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                           </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mFaceAnalysisDIY warmup time: \u001b[1;36m0.\u001b[0m246s                                              \u001b]8;id=144078;file:///content/LivePortrait/src/utils/face_analysis_diy.py\u001b\\\u001b[2mface_analysis_diy.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=993557;file:///content/LivePortrait/src/utils/face_analysis_diy.py#79\u001b\\\u001b[2m79\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>FaceAnalysisDIY warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>246s                                              <a href=\"file:///content/LivePortrait/src/utils/face_analysis_diy.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">face_analysis_diy.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/utils/face_analysis_diy.py#79\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">79</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLandmarkRunner warmup time: \u001b[1;36m0.\u001b[0m155s                                           \u001b]8;id=719262;file:///content/LivePortrait/src/utils/human_landmark_runner.py\u001b\\\u001b[2mhuman_landmark_runner.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=469898;file:///content/LivePortrait/src/utils/human_landmark_runner.py#95\u001b\\\u001b[2m95\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>LandmarkRunner warmup time: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.</span>155s                                           <a href=\"file:///content/LivePortrait/src/utils/human_landmark_runner.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">human_landmark_runner.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/utils/human_landmark_runner.py#95\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">95</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad source image from \u001b[35m/content/LivePortrait/assets/examples/source/\u001b[0m\u001b[95ms9.jpg\u001b[0m  \u001b]8;id=737625;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=452714;file:///content/LivePortrait/src/live_portrait_pipeline.py#90\u001b\\\u001b[2m90\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Load source image from <span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/assets/examples/source/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">s9.jpg</span>  <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#90\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">90</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mLoad driving video from:                                                   \u001b]8;id=820602;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=210015;file:///content/LivePortrait/src/live_portrait_pipeline.py#133\u001b\\\u001b[2m133\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/assets/examples/driving/\u001b[0m\u001b[95md0.mp4\u001b[0m, FPS is \u001b[1;36m25\u001b[0m            \u001b[2m                             \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Load driving video from:                                                   <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#133\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">133</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/assets/examples/driving/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">d0.mp4</span>, FPS is <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25</span>            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:19:40]\u001b[0m\u001b[2;36m \u001b[0mStart making driving motion template\u001b[33m...\u001b[0m                                    \u001b]8;id=50711;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=242931;file:///content/LivePortrait/src/live_portrait_pipeline.py#144\u001b\\\u001b[2m144\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:19:40] </span>Start making driving motion template<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                                    <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#144\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">144</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25061151c9a7419dbcf28b25f5f917b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:19:58]\u001b[0m\u001b[2;36m \u001b[0mDump motion template to                                                    \u001b]8;id=573530;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=508961;file:///content/LivePortrait/src/live_portrait_pipeline.py#172\u001b\\\u001b[2m172\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[35m/content/LivePortrait/assets/examples/driving/\u001b[0m\u001b[95md0.pkl\u001b[0m                       \u001b[2m                             \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:19:58] </span>Dump motion template to                                                    <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#172\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #800080; text-decoration-color: #800080\">/content/LivePortrait/assets/examples/driving/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">d0.pkl</span>                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mPrepared pasteback mask done.                                              \u001b]8;id=859118;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=332531;file:///content/LivePortrait/src/live_portrait_pipeline.py#183\u001b\\\u001b[2m183\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Prepared pasteback mask done.                                              <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#183\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">183</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:19:59]\u001b[0m\u001b[2;36m \u001b[0mThe animated video consists of \u001b[1;36m78\u001b[0m frames.                                  \u001b]8;id=766191;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=365062;file:///content/LivePortrait/src/live_portrait_pipeline.py#270\u001b\\\u001b[2m270\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:19:59] </span>The animated video consists of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">78</span> frames.                                  <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#270\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">270</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37a72a5a651b472ca1a132146e3d625b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ad6c5160bcd49e08138966b7f9f1530"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87c69c0018b143c88ea80da0ed25a862"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9aa42d3e610e4053a9c7f5b4c82ce118"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m[05:20:18]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;32mAnimated template: \u001b[0m\u001b[1;35m/content/LivePortrait/assets/examples/driving/\u001b[0m\u001b[1;95md0.pkl\u001b[0m\u001b[1;32m,  \u001b[0m \u001b]8;id=132926;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=334611;file:///content/LivePortrait/src/live_portrait_pipeline.py#503\u001b\\\u001b[2m503\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m           \u001b[0m\u001b[1;32myou can specify `-d` argument with this template path next time to avoid  \u001b[0m \u001b[2m                             \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m\u001b[1;32mcropping video, motion making and protecting privacy.                     \u001b[0m \u001b[2m                             \u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05:20:18] </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Animated template: </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">/content/LivePortrait/assets/examples/driving/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">d0.pkl</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">,  </span> <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#503\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">503</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">you can specify `-d` argument with this template path next time to avoid  </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">cropping video, motion making and protecting privacy.                     </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                             </span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mAnimated video: animations/s9--d0.mp4                                      \u001b]8;id=539288;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=603762;file:///content/LivePortrait/src/live_portrait_pipeline.py#504\u001b\\\u001b[2m504\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Animated video: animations/s9--d0.mp4                                      <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#504\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">504</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mAnimated video with concat: animations/s9--d0_concat.mp4                   \u001b]8;id=824523;file:///content/LivePortrait/src/live_portrait_pipeline.py\u001b\\\u001b[2mlive_portrait_pipeline.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=64463;file:///content/LivePortrait/src/live_portrait_pipeline.py#505\u001b\\\u001b[2m505\u001b[0m\u001b]8;;\u001b\\\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>Animated video with concat: animations/s9--d0_concat.mp4                   <a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">live_portrait_pipeline.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///content/LivePortrait/src/live_portrait_pipeline.py#505\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">505</span></a>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Elapsed inference time (AMP only): 38124 ms total ===\n"
          ]
        }
      ]
    }
  ]
}